{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8488a4b-b2f2-4e9f-a91d-55aad7ecbac4",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a14a1-b1ad-47b8-9338-316d7f87afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import system libraries\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "# import data handling tools\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "# import Deep learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('required modules are loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531b5b0-3482-43eb-9a50-c7a11c1d3ba2",
   "metadata": {},
   "source": [
    "Load train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb238bbd-4934-4108-8934-22ccfe41e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file = pd.read_csv('./Training_set.csv')\n",
    "test_csv_file = pd.read_csv('./Testing_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736370f-6995-4e28-b19a-44fe1b0cef4a",
   "metadata": {},
   "source": [
    "Create fucntions for evaluate the dataset and call functions to get an idea about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2920a-db34-4e20-9d15-9d55869dcee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_of_the_dataset(dataframe, dataset_name='dataframe'):\n",
    "    print(f\"{dataset_name} dataset has {dataframe.shape[0]} rows and {dataframe.shape[1]} columns\")\n",
    "    \n",
    "def check_null_values(dataframe, dataset_name='dataframe'):\n",
    "    print(f\"Number of null values in the {dataset_name} dataset: \")\n",
    "    print(dataframe.isnull().sum())\n",
    "\n",
    "def check_unique_values(dataframe, dataset_name='dataframe'):\n",
    "    print(f\"Number of unique values in {dataset_name} dataset: \")\n",
    "    print(dataframe.nunique())\n",
    "    \n",
    "def seperator(sep=50):\n",
    "    print(\"-\"*sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578307ff-0253-4321-86b4-5058819e39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3ccce-0c5a-446a-95d9-ec5391747c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90851fa0-7fa5-41a9-b609-ffb03d56a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_of_the_dataset(train_csv_file, \"Train\")\n",
    "shape_of_the_dataset(test_csv_file, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ea092-e86e-4a6d-8d79-72d6acbb6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null_values(train_csv_file, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82974795-b428-421f-9ab3-5a1d3f5a0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null_values(test_csv_file, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51284318-01f0-42a0-8aa3-7be764f38669",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unique_values(train_csv_file, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e6530-69bd-47a1-b708-05d820cffdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unique_values(test_csv_file, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ac968-b655-4495-9a0b-78a19bd408a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(train_csv_file)\n",
    "plt.title('Missing Values Distribution in the Training dataset', fontsize=28, fontstyle='oblique');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319ca8f-fdd2-4113-848a-6e9b2b796242",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(est_csv_file)\n",
    "plt.title('Missing Values Distribution in the Testing dataset', fontsize=28, fontstyle='oblique');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57bb77-3e1d-47b6-a836-4fc96af0d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file['filename'] = './dataset/train/' + train_csv_file['filename']\n",
    "test_csv_file['filename'] = './dataset/test/' + test_csv_file['filename']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be07eb-42b9-4063-9cd8-52a797f24532",
   "metadata": {},
   "source": [
    "Train and test data set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a945ca3-4e9f-49a3-a101-450dc256a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_size = 0.80\n",
    "train_dataframe, valid_dataframe = train_test_split(train_csv_file,  train_size= train_dataset_size, shuffle= True, random_state= 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2a537-3b9a-4e2d-ac0f-6bcca515de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce59a53-c038-4c9b-8ae1-ada8ff335274",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e1293-2995-4747-ba41-cd54ea0d4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_of_the_dataset(train_dataframe, \"Train\")\n",
    "shape_of_the_dataset(valid_dataframe, \"Valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d50af-0fe7-4eef-80f2-dfdac1cafc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null_values(train_dataframe, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8534cf-4066-4769-bff9-dc382cc2d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null_values(valid_dataframe, \"Valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62796396-4f1f-4020-b725-711f705f923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unique_values(train_dataframe, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ebeab-5109-4585-bfd8-879e638244ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unique_values(valid_dataframe, \"Valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebffb8-0aa7-4b9c-b9fb-1e4e868bda63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_count_plot(x, dataframe, title, xlabel, ylabel, width, height, order = None, rotation=False, palette='winter', hue=None):\n",
    "    count = len(dataframe)\n",
    "    plt.figure(figsize=(width,height))\n",
    "    ax = sns.countplot(x = x, palette=palette, order = order, hue=hue)\n",
    "    plt.title(title, fontsize=20)\n",
    "    if rotation:\n",
    "        plt.xticks(rotation = 'vertical')\n",
    "    plt.xlabel(xlabel, fontsize=16)\n",
    "    plt.ylabel(ylabel, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49dfee-ce58-4933-8f6b-6eecd2469c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_order = train_dataframe['label'].value_counts()\n",
    "train__ds_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff70827-4f43-492b-9c43-6e04012b1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_dataframe['label']\n",
    "class_count_plot(x, train_dataframe, \"Class Distrbution in the Training dataset\", 'Class', 'Frequency', 15,10, order = train_order.index, rotation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9468c-a752-41ba-9ddb-27b1226376e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds_order = valid_dataframe['label'].value_counts()\n",
    "valid_ds_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fcb59d-25c4-42ac-93c0-ef9efca3db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = valid_dataframe['label']\n",
    "class_count_plot(x, valid_dataframe, \"Class Distrbution in the Validation dataset\", 'Class', 'Frequency', 15,10, order = valid_order.index, rotation=True, palette='autumn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5ee74-285a-4911-bf3b-b582d7707f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size = (224, 224)\n",
    "channels = 3\n",
    "image_shape = (image_size[0], image_size[1], channels)\n",
    "\n",
    "test_length = len(test_csv_file)\n",
    "test_batch_size = max(sorted([test_length // n for n in range(1, test_length + 1) if test_length%n == 0 and test_length/n <= 80]))\n",
    "total_test_steps = ts_length // test_batch_size\n",
    "\n",
    "print(test_length)\n",
    "print(test_batch_size)\n",
    "print(total_test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23444fa7-54b3-401a-9fdd-3bb9b24bf1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe['filename'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4e29b-4cd6-4bb9-9db5-d36967b021c9",
   "metadata": {},
   "source": [
    "data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf8c6b-9d11-42ea-9681-55ccafd51871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar(img):\n",
    "    return img\n",
    "\n",
    "t_gen = ImageDataGenerator(preprocessing_function= scalar,\n",
    "                            rotation_range=40,\n",
    "                            width_shift_range=0.3,\n",
    "                            height_shift_range=0.2,\n",
    "                            brightness_range=None,\n",
    "                            shear_range=0.1,\n",
    "                            zoom_range=0.3,\n",
    "                            channel_shift_range=0.4)\n",
    "\n",
    "tst_gen = ImageDataGenerator(preprocessing_function= scalar,\n",
    "                            rotation_range=40,\n",
    "                            width_shift_range=0.3,\n",
    "                            height_shift_range=0.2,\n",
    "                            brightness_range=None,\n",
    "                            shear_range=0.1,\n",
    "                            zoom_range=0.3,\n",
    "                            channel_shift_range=0.4)\n",
    "\n",
    "train_gen = t_gen.flow_from_dataframe( train_df, \n",
    "                                        x_col= 'filename', \n",
    "                                        y_col= 'label', \n",
    "                                        target_size= img_size, \n",
    "                                        class_mode= 'categorical',\n",
    "                                        color_mode= 'rgb', \n",
    "                                        shuffle= False, \n",
    "                                        batch_size= batch_size)\n",
    "\n",
    "valid_gen = tst_gen.flow_from_dataframe( valid_df, \n",
    "                                       x_col= 'filename', \n",
    "                                       y_col= 'label', \n",
    "                                       target_size= img_size, \n",
    "                                       class_mode= 'categorical',\n",
    "                                       color_mode= 'rgb', \n",
    "                                       shuffle= False, \n",
    "                                       batch_size= batch_size)\n",
    "test_gen = tst_gen.flow_from_dataframe( test_csv_file, \n",
    "                                       x_col= 'filename', \n",
    "                                       y_col= 'label', \n",
    "                                       target_size= img_size, \n",
    "                                       class_mode= 'categorical',\n",
    "                                       color_mode= 'rgb', \n",
    "                                       shuffle= False, \n",
    "                                       batch_size= batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedaa363-f1b8-46e9-9972-5d9ad671d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dic = train_gen.class_indices      # defines dictionary {'class': index}\n",
    "classes = list(get_dic.keys())       # defines list of dictionary's kays (classes), classes names : string\n",
    "imgs, lbls = next(train_gen)      # get a batch size samples from the generator\n",
    "\n",
    "plt.figure(figsize= (20, 20))\n",
    "\n",
    "for i in range(24):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    img = imgs[i] / 255       # scales data to range (0 - 255)\n",
    "    plt.imshow(img)\n",
    "    index = np.argmax(lbls[i])  # get image index\n",
    "    class_name = classes[index]   # get class of image\n",
    "    plt.title(class_name, color= 'blue', fontsize= 12)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae12210c-d82f-4e12-b51e-4e33234eb62c",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84fb6a9-025b-435c-866a-bf39161a42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model Structure\n",
    "#from keras.applications import EfficientNetB0\n",
    "image_size = (224, 224)\n",
    "channels = 3\n",
    "image_shape = (image_size[0], image_size[1], channels)\n",
    "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
    "\n",
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "base_model = tf.keras.applications.efficientnet.EfficientNetB5(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "#base_model = efc.EfficientNetB5(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
    "    Dense(512, activation = 'relu'),\n",
    "    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
    "                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
    "    Dropout(rate= 0.45, seed= 123),\n",
    "    Dense(class_count, activation= 'softmax')\n",
    "])\n",
    "model.build([None, 224, 224, 3])\n",
    "\n",
    "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a93b80-9cf3-4cd4-bf17-26872bbcafdc",
   "metadata": {},
   "source": [
    "define early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493cdf3-d15b-4bc7-ada4-56e2df173422",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', \n",
    "                               patience=5, \n",
    "                               restore_best_weights=True,\n",
    "                               mode='max',\n",
    "                              )\n",
    "\n",
    "def step_decay(epoch):\n",
    "    \n",
    "     initial_lrate = 0.1\n",
    "     drop = 0.5\n",
    "     epochs_drop = 10.0\n",
    "     learning_rate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "     return learning_rate\n",
    "\n",
    "learn__rate_scheduler = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d77b766-bd3f-4613-9481-35ddc9686e5e",
   "metadata": {},
   "source": [
    "run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19fc9b-f0e4-4de0-8750-f6ccfec8858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16   # set batch size for training\n",
    "epochs = 10   # number of all epochs in training\n",
    "\n",
    "history = model.fit(x= train_gen, \n",
    "                    epochs= epochs, \n",
    "                    verbose= 1, \n",
    "                    validation_data= valid_gen, \n",
    "                    validation_steps= None, \n",
    "                    shuffle= False,\n",
    "                    batch_size = batch_size,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714ecd2-e19f-4f6b-9eb0-de7ac1a0ca53",
   "metadata": {},
   "source": [
    "evaluate training of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce4f1c-78a7-4e05-bb7f-7ff570dc92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define needed variables\n",
    "train_accuracy = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "validation_loss = history.history['val_loss']\n",
    "index_loss = np.argmin(validation_loss)\n",
    "validation_lowest = validation_loss[index_loss]\n",
    "index_accuracy = np.argmax(validation_accuracy)\n",
    "accuracy_highest = validation_accuracy[index_accuracy]\n",
    "Epochs = [i+1 for i in range(len(train_accuracy))]\n",
    "loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "accuracy_label = f'best epoch= {str(index_accuracy + 1)}'\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize= (20, 8))\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(Epochs, train_loss, 'r', label= 'Training loss')\n",
    "plt.plot(Epochs, validation_loss, 'g', label= 'Validation loss')\n",
    "plt.scatter(index_loss + 1, validation_lowest, s= 150, c= 'blue', label= loss_label)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(Epochs, train_accuracy, 'r', label= 'Training Accuracy')\n",
    "plt.plot(Epochs, validation_accuracy, 'g', label= 'Validation Accuracy')\n",
    "plt.scatter(index_accuracy + 1 , accuracy_highest, s= 150, c= 'blue', label= accuracy_label)\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c760281-d386-48fc-af10-44f13eacc511",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
    "validation_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", training_score[0])\n",
    "print(\"Train Accuracy: \", training_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Validation Loss: \", validation_score[0])\n",
    "print(\"Validation Accuracy: \", validation_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3d43f-4b52-4c5a-92c2-7a6dc0dd4933",
   "metadata": {},
   "source": [
    "save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325ac3c-6d08-409b-a3be-dd5f32488b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"butterfly_efficientnet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd22f67-f084-45cc-bb18-a61035d81b3d",
   "metadata": {},
   "source": [
    "evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1863f0-5823-48e8-8ab6-701a8654fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_gen)\n",
    "print(f'test accuracy : {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc611544-6db5-4c3d-a7f8-b4b84cd9c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_on_test_data(test_gen):    \n",
    "    y_prediction= []\n",
    "    error_list=[]\n",
    "    error_prediction_list = []\n",
    "    y_true=test_gen.labels\n",
    "    classes=list(test_gen.class_indices.keys())\n",
    "    class_count=len(classes)\n",
    "    errors=0\n",
    "    predictions=model.predict(test_gen, verbose=1)\n",
    "    tests=len(predictions)    \n",
    "    for i, p in enumerate(predictions):        \n",
    "        prediction_index=np.argmax(p)         \n",
    "        true_index=test_gen.labels[i]  # labels are integer values        \n",
    "        if prediction_index != true_index: # a misclassification has occurred                                           \n",
    "            errors=errors + 1\n",
    "            file=test_gen.filenames[i]\n",
    "            error_list.append(file)\n",
    "            error_class=classes[prediction_index]\n",
    "            error_prediction_list.append(error_class)\n",
    "        y_prediction.append(prediction_index)\n",
    "            \n",
    "    accuracy=( 1-errors/tests) * 100\n",
    "    message=f'there were {errors} errors in {tests} tests for an accuracy of {acc:6.2f}'\n",
    "    print(message) \n",
    "    yprediction=np.array(y_prediction)\n",
    "    ytrue=np.array(y_true)\n",
    "    f1score=f1_score(ytrue, yprediction, average='weighted')* 100\n",
    "    if class_count <=75:\n",
    "        cm = confusion_matrix(ytrue, yprediction )\n",
    "        # plot the confusion matrix\n",
    "        plt.figure(figsize=(500, 500))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "        plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n",
    "        plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "    classification_rep = classification_report(y_true, y_pred, target_names=classes, digits= 4) # create classification report\n",
    "    print(\"Classification Report:\\n----------------------\\n\", classification_rep)\n",
    "    return errors, tests, error_list, error_prediction_list, f1score\n",
    "\n",
    "errors, tests, error_list, error_prediction_list, f1score =prediction_on_test_data(test_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
